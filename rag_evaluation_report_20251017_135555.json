{
  "evaluation_metadata": {
    "evaluation_date": "2025-10-17T18:12:56.267096",
    "test_dataset_path": "test_dataset.json",
    "total_test_questions": 200,
    "questions_evaluated": 200
  },
  "overall_metrics": {
    "total_questions_evaluated": 200,
    "valid_evaluations": 200,
    "error_rate": 0.0,
    "semantic_similarity": {
      "mean": 5.48,
      "std": 1.153082824431966,
      "min": 2.0,
      "max": 6.0
    },
    "factual_accuracy": {
      "mean": 5.37,
      "std": 1.9008156144139807,
      "min": 1.0,
      "max": 8.0
    },
    "completeness": {
      "mean": 4.295,
      "std": 0.998986986902232,
      "min": 0.0,
      "max": 5.0
    },
    "relevance": {
      "mean": 6.44,
      "std": 1.3101144988129856,
      "min": 0.0,
      "max": 7.0
    },
    "overall_score": {
      "mean": 5.402,
      "std": 1.1431080438873658,
      "min": 0.8999999999999999,
      "max": 6.6
    },
    "source_precision": 0.72,
    "source_recall": 0.72,
    "category_accuracy": 0.72,
    "average_evaluation_time": 61.412881784999996,
    "total_evaluation_time": 12282.576357,
    "score_distribution": {
      "excellent_9_10": 0.0,
      "good_7_8": 0.0,
      "fair_5_6": 79.5,
      "poor_3_4": 16.0,
      "very_poor_0_2": 4.5
    }
  },
  "category_breakdown": {
    "Toxics": {
      "count": 24,
      "semantic_similarity": [
        6.0,
        4.0,
        6.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        2.0,
        6.0,
        6.0,
        4.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        8.0,
        3.0,
        8.0,
        4.0,
        4.0,
        4.0,
        4.0,
        8.0,
        8.0,
        4.0,
        4.0,
        4.0,
        6.0,
        1.0,
        4.0,
        8.0,
        8.0,
        8.0,
        4.0,
        1.0,
        4.0,
        8.0,
        4.0,
        8.0
      ],
      "completeness": [
        5.0,
        2.0,
        5.0,
        3.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        3.0,
        5.0,
        5.0,
        3.0,
        3.0,
        5.0,
        5.0,
        4.0,
        5.0,
        5.0,
        0.0,
        3.0,
        5.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        5.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        5.0,
        4.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        0.0,
        5.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 19,
      "avg_semantic_similarity": 5.166666666666667,
      "avg_factual_accuracy": 5.291666666666667,
      "avg_completeness": 4.208333333333333,
      "avg_relevance": 6.166666666666667,
      "source_precision": 0.7916666666666666
    },
    "Climate": {
      "count": 63,
      "semantic_similarity": [
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0
      ],
      "factual_accuracy": [
        8.0,
        8.0,
        6.0,
        5.0,
        5.0,
        5.0,
        5.0,
        8.0,
        8.0,
        5.0,
        5.0,
        5.0,
        4.0,
        8.0,
        8.0,
        5.0,
        8.0,
        5.0,
        8.0,
        8.0,
        1.0,
        5.0,
        5.0,
        8.0,
        4.0,
        8.0,
        6.0,
        4.0,
        4.0,
        5.0,
        5.0,
        1.0,
        5.0,
        8.0,
        8.0,
        5.0,
        4.0,
        4.0,
        8.0,
        4.0,
        4.0,
        4.0,
        8.0,
        5.0,
        3.0,
        8.0,
        8.0,
        8.0,
        8.0,
        4.0,
        3.0,
        5.0,
        4.0,
        8.0,
        4.0,
        8.0,
        6.0,
        5.0,
        4.0,
        5.0,
        5.0,
        6.0,
        8.0
      ],
      "completeness": [
        5.0,
        5.0,
        3.0,
        4.0,
        4.0,
        4.0,
        4.0,
        4.0,
        5.0,
        4.0,
        4.0,
        4.0,
        5.0,
        5.0,
        5.0,
        4.0,
        5.0,
        4.0,
        5.0,
        5.0,
        3.0,
        4.0,
        4.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        4.0,
        4.0,
        4.0,
        4.0,
        5.0,
        5.0,
        4.0,
        3.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        4.0,
        2.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        2.0,
        4.0,
        5.0,
        5.0,
        5.0,
        5.0,
        3.0,
        4.0,
        5.0,
        4.0,
        4.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        2.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        3.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 48,
      "avg_semantic_similarity": 5.682539682539683,
      "avg_factual_accuracy": 5.714285714285714,
      "avg_completeness": 4.428571428571429,
      "avg_relevance": 6.698412698412699,
      "source_precision": 0.7619047619047619
    },
    "Forests": {
      "count": 23,
      "semantic_similarity": [
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        8.0,
        8.0,
        4.0,
        5.0,
        4.0,
        4.0,
        8.0,
        5.0,
        5.0,
        4.0,
        8.0,
        5.0,
        6.0,
        8.0,
        5.0,
        4.0,
        4.0,
        6.0,
        4.0,
        8.0,
        4.0,
        4.0,
        8.0
      ],
      "completeness": [
        5.0,
        5.0,
        3.0,
        4.0,
        5.0,
        3.0,
        5.0,
        4.0,
        4.0,
        5.0,
        5.0,
        4.0,
        3.0,
        5.0,
        4.0,
        5.0,
        5.0,
        3.0,
        5.0,
        5.0,
        3.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0
      ],
      "source_matches": 12,
      "avg_semantic_similarity": 5.826086956521739,
      "avg_factual_accuracy": 5.608695652173913,
      "avg_completeness": 4.3478260869565215,
      "avg_relevance": 6.565217391304348,
      "source_precision": 0.5217391304347826
    },
    "Nuclear": {
      "count": 6,
      "semantic_similarity": [
        4.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        6.0,
        5.0,
        4.0,
        6.0,
        5.0,
        8.0
      ],
      "completeness": [
        5.0,
        4.0,
        5.0,
        3.0,
        4.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0
      ],
      "source_matches": 6,
      "avg_semantic_similarity": 5.333333333333333,
      "avg_factual_accuracy": 5.666666666666667,
      "avg_completeness": 4.333333333333333,
      "avg_relevance": 6.666666666666667,
      "source_precision": 1.0
    },
    "Oceans": {
      "count": 16,
      "semantic_similarity": [
        2.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        1.0,
        5.0,
        5.0,
        4.0,
        6.0,
        4.0,
        8.0,
        8.0,
        4.0,
        4.0,
        4.0,
        8.0,
        8.0,
        5.0,
        5.0,
        4.0
      ],
      "completeness": [
        3.0,
        4.0,
        4.0,
        5.0,
        3.0,
        5.0,
        5.0,
        5.0,
        5.0,
        3.0,
        5.0,
        5.0,
        5.0,
        4.0,
        4.0,
        5.0
      ],
      "relevance": [
        4.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 12,
      "avg_semantic_similarity": 5.625,
      "avg_factual_accuracy": 5.1875,
      "avg_completeness": 4.375,
      "avg_relevance": 6.5625,
      "source_precision": 0.75
    },
    "Brazilian Amazon": {
      "count": 8,
      "semantic_similarity": [
        6.0,
        4.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        4.0,
        6.0,
        3.0,
        8.0,
        8.0,
        4.0,
        5.0,
        4.0
      ],
      "completeness": [
        3.0,
        3.0,
        2.0,
        5.0,
        5.0,
        5.0,
        4.0,
        5.0
      ],
      "relevance": [
        5.0,
        5.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 5,
      "avg_semantic_similarity": 5.5,
      "avg_factual_accuracy": 5.25,
      "avg_completeness": 4.0,
      "avg_relevance": 6.25,
      "source_precision": 0.625
    },
    "Fossil Fuels": {
      "count": 22,
      "semantic_similarity": [
        6.0,
        6.0,
        2.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0,
        2.0,
        6.0,
        2.0,
        4.0,
        4.0,
        4.0,
        2.0
      ],
      "factual_accuracy": [
        8.0,
        5.0,
        1.0,
        5.0,
        4.0,
        4.0,
        8.0,
        5.0,
        5.0,
        5.0,
        6.0,
        4.0,
        4.0,
        4.0,
        4.0,
        3.0,
        5.0,
        3.0,
        6.0,
        6.0,
        3.0,
        3.0
      ],
      "completeness": [
        5.0,
        4.0,
        0.0,
        4.0,
        5.0,
        5.0,
        5.0,
        4.0,
        4.0,
        4.0,
        3.0,
        5.0,
        5.0,
        5.0,
        5.0,
        4.0,
        4.0,
        4.0,
        3.0,
        5.0,
        2.0,
        4.0
      ],
      "relevance": [
        7.0,
        7.0,
        0.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        5.0,
        5.0,
        7.0,
        5.0,
        5.0
      ],
      "source_matches": 12,
      "avg_semantic_similarity": 4.909090909090909,
      "avg_factual_accuracy": 4.590909090909091,
      "avg_completeness": 4.045454545454546,
      "avg_relevance": 6.136363636363637,
      "source_precision": 0.5454545454545454
    },
    "Greenpeace": {
      "count": 1,
      "semantic_similarity": [
        2.0
      ],
      "factual_accuracy": [
        1.0
      ],
      "completeness": [
        3.0
      ],
      "relevance": [
        1.0
      ],
      "source_matches": 1,
      "avg_semantic_similarity": 2.0,
      "avg_factual_accuracy": 1.0,
      "avg_completeness": 3.0,
      "avg_relevance": 1.0,
      "source_precision": 1.0
    },
    "Sustainable Seafood": {
      "count": 11,
      "semantic_similarity": [
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        6.0,
        4.0,
        6.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        4.0,
        4.0,
        4.0,
        4.0,
        4.0,
        8.0,
        6.0,
        4.0,
        8.0,
        8.0,
        4.0
      ],
      "completeness": [
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        5.0,
        3.0,
        5.0,
        5.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        7.0,
        5.0,
        7.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 7,
      "avg_semantic_similarity": 5.818181818181818,
      "avg_factual_accuracy": 5.2727272727272725,
      "avg_completeness": 4.818181818181818,
      "avg_relevance": 6.818181818181818,
      "source_precision": 0.6363636363636364
    },
    "Energy Transfer Lawsuit": {
      "count": 2,
      "semantic_similarity": [
        6.0,
        6.0
      ],
      "factual_accuracy": [
        4.0,
        5.0
      ],
      "completeness": [
        5.0,
        4.0
      ],
      "relevance": [
        7.0,
        7.0
      ],
      "source_matches": 2,
      "avg_semantic_similarity": 6.0,
      "avg_factual_accuracy": 4.5,
      "avg_completeness": 4.5,
      "avg_relevance": 7.0,
      "source_precision": 1.0
    },
    "Indonesian Rainforests": {
      "count": 4,
      "semantic_similarity": [
        4.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        6.0,
        5.0,
        5.0,
        5.0
      ],
      "completeness": [
        5.0,
        4.0,
        4.0,
        4.0
      ],
      "relevance": [
        7.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 4,
      "avg_semantic_similarity": 5.5,
      "avg_factual_accuracy": 5.25,
      "avg_completeness": 4.25,
      "avg_relevance": 7.0,
      "source_precision": 1.0
    },
    "Plastics & Health": {
      "count": 2,
      "semantic_similarity": [
        6.0,
        6.0
      ],
      "factual_accuracy": [
        8.0,
        8.0
      ],
      "completeness": [
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0
      ],
      "source_matches": 1,
      "avg_semantic_similarity": 6.0,
      "avg_factual_accuracy": 8.0,
      "avg_completeness": 5.0,
      "avg_relevance": 7.0,
      "source_precision": 0.5
    },
    "Bitcoin": {
      "count": 2,
      "semantic_similarity": [
        4.0,
        6.0
      ],
      "factual_accuracy": [
        6.0,
        4.0
      ],
      "completeness": [
        3.0,
        5.0
      ],
      "relevance": [
        5.0,
        7.0
      ],
      "source_matches": 2,
      "avg_semantic_similarity": 5.0,
      "avg_factual_accuracy": 5.0,
      "avg_completeness": 4.0,
      "avg_relevance": 6.0,
      "source_precision": 1.0
    },
    "Agriculture": {
      "count": 7,
      "semantic_similarity": [
        6.0,
        6.0,
        4.0,
        2.0,
        6.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        8.0,
        5.0,
        6.0,
        1.0,
        5.0,
        8.0,
        8.0
      ],
      "completeness": [
        5.0,
        4.0,
        5.0,
        0.0,
        4.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        7.0,
        0.0,
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 7,
      "avg_semantic_similarity": 5.142857142857143,
      "avg_factual_accuracy": 5.857142857142857,
      "avg_completeness": 4.0,
      "avg_relevance": 6.0,
      "source_precision": 1.0
    },
    "Canadian Boreal": {
      "count": 3,
      "semantic_similarity": [
        2.0,
        6.0,
        6.0
      ],
      "factual_accuracy": [
        3.0,
        4.0,
        4.0
      ],
      "completeness": [
        1.0,
        3.0,
        3.0
      ],
      "relevance": [
        1.0,
        5.0,
        5.0
      ],
      "source_matches": 2,
      "avg_semantic_similarity": 4.666666666666667,
      "avg_factual_accuracy": 3.6666666666666665,
      "avg_completeness": 2.3333333333333335,
      "avg_relevance": 3.6666666666666665,
      "source_precision": 0.6666666666666666
    },
    "Voting Rights": {
      "count": 1,
      "semantic_similarity": [
        6.0
      ],
      "factual_accuracy": [
        4.0
      ],
      "completeness": [
        3.0
      ],
      "relevance": [
        5.0
      ],
      "source_matches": 0,
      "avg_semantic_similarity": 6.0,
      "avg_factual_accuracy": 4.0,
      "avg_completeness": 3.0,
      "avg_relevance": 5.0,
      "source_precision": 0.0
    },
    "Myth of Recycling": {
      "count": 2,
      "semantic_similarity": [
        6.0,
        6.0
      ],
      "factual_accuracy": [
        5.0,
        8.0
      ],
      "completeness": [
        4.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0
      ],
      "source_matches": 1,
      "avg_semantic_similarity": 6.0,
      "avg_factual_accuracy": 6.5,
      "avg_completeness": 4.5,
      "avg_relevance": 7.0,
      "source_precision": 0.5
    },
    "Plastics": {
      "count": 3,
      "semantic_similarity": [
        6.0,
        4.0,
        6.0
      ],
      "factual_accuracy": [
        4.0,
        6.0,
        4.0
      ],
      "completeness": [
        5.0,
        5.0,
        5.0
      ],
      "relevance": [
        7.0,
        7.0,
        7.0
      ],
      "source_matches": 3,
      "avg_semantic_similarity": 5.333333333333333,
      "avg_factual_accuracy": 4.666666666666667,
      "avg_completeness": 5.0,
      "avg_relevance": 7.0,
      "source_precision": 1.0
    }
  },
  "sample_results": [
    {
      "question_id": "qa_0001",
      "question": "What are the underlying consequences for human health when chemical industries prioritize profits over safety and regulatory compliance?",
      "ground_truth": "Unfortunately, the provided paragraph does not explicitly state the underlying consequences for human health when chemical industries prioritize profits over safety and regulatory compliance. However,...",
      "rag_answer": "**The Underlying Consequences of Prioritizing Profits over Safety and Regulatory Compliance**\n\nThe provided context reveals the consequences of chemical industries prioritizing profits over safety and...",
      "scores": {
        "semantic_similarity": 6.0,
        "factual_accuracy": 8.0,
        "completeness": 5.0,
        "relevance": 7.0
      },
      "source_match": true,
      "category": "Toxics"
    },
    {
      "question_id": "qa_0002",
      "question": "What role do the 26 key banks play in financing the expansion of Canada's tar sands industry, and what are the potential consequences for global climate efforts?",
      "ground_truth": "The 26 key banks play a significant role in financing the expansion of Canada's tar sands industry by lending money to companies building pipelines that transport tar sands to market. This financial s...",
      "rag_answer": "**The Role of 26 Key Banks in Financing the Expansion of Canada's Tar Sands Industry**\n\nGreenpeace research has identified 26 key banks that are lending money to companies building tar sands pipelines...",
      "scores": {
        "semantic_similarity": 6.0,
        "factual_accuracy": 8.0,
        "completeness": 5.0,
        "relevance": 7.0
      },
      "source_match": true,
      "category": "Climate"
    },
    {
      "question_id": "qa_0003",
      "question": "What are the environmental consequences of increasing highway expansion and maintenance in the United States?",
      "ground_truth": "Unfortunately, The provided information is insufficient to answer completely. The paragraph only mentions specific dollar amounts for highway expansion ($3.0 billion) and maintenance & repair ($472 mi...",
      "rag_answer": "**Environmental Consequences of Increasing Highway Expansion and Maintenance in the United States**\n\nThe provided context consists of eight documents that analyze the greenhouse gas (GHG) emissions im...",
      "scores": {
        "semantic_similarity": 6.0,
        "factual_accuracy": 8.0,
        "completeness": 5.0,
        "relevance": 7.0
      },
      "source_match": true,
      "category": "Climate"
    },
    {
      "question_id": "qa_0004",
      "question": "What would be the environmental cost of a nuclear fuel cycle in Russia compared to other countries listed in the Nuclear Map?",
      "ground_truth": "The provided information is insufficient to accurately determine the environmental cost of a nuclear fuel cycle in Russia compared to other countries listed in the Nuclear Map. The paragraph only ment...",
      "rag_answer": "**Environmental Cost of Nuclear Fuel Cycle**\n\nThe environmental cost of a nuclear fuel cycle is a complex issue that involves various aspects, including the production, transportation, and disposal of...",
      "scores": {
        "semantic_similarity": 4.0,
        "factual_accuracy": 6.0,
        "completeness": 3.0,
        "relevance": 5.0
      },
      "source_match": true,
      "category": "Climate"
    },
    {
      "question_id": "qa_0005",
      "question": "What would be a more effective approach for President Obama to show leadership on global warming in this context?",
      "ground_truth": "Based on the provided information, a more effective approach for President Obama to show leadership on global warming would be to directly respond to Greenpeace's challenge. By doing so, he would demo...",
      "rag_answer": "**A More Effective Approach for President Obama to Show Leadership on Global Warming**\n\nTo address the question of what would be a more effective approach for President Obama to show leadership on glo...",
      "scores": {
        "semantic_similarity": 6.0,
        "factual_accuracy": 5.0,
        "completeness": 4.0,
        "relevance": 7.0
      },
      "source_match": true,
      "category": "Climate"
    }
  ]
}