# RAG Testing and Evaluation System

Sistema completo de testing y evaluaci√≥n para el sistema RAG de Greenpeace. Incluye generaci√≥n autom√°tica de datasets de prueba, evaluaci√≥n con LLM juez y m√©tricas de rendimiento detalladas.

## üéØ Caracter√≠sticas Principales

### 1. Generaci√≥n de Dataset de Prueba (`rag_test_generator.py`)
- **Extracci√≥n inteligente de p√°rrafos**: Selecciona p√°rrafos relevantes de documentos del dataset
- **Generaci√≥n de preguntas contextuales**: Usa Gemini para crear preguntas no literales
- **Respuestas de referencia (Ground Truth)**: Genera respuestas basadas en el contexto original
- **Metadata completa**: Guarda referencia al documento fuente, categor√≠a, etc.
- **Configuraci√≥n flexible**: 50-1000 preguntas seg√∫n necesidades

### 2. Sistema de Evaluaci√≥n (`rag_evaluator.py`)
- **LLM Juez**: Evaluaci√≥n sem√°ntica con Gemini 2.0 Flash
- **M√©tricas multidimensionales**:
  - Similitud sem√°ntica (0-10)
  - Precisi√≥n factual (0-10)
  - Completitud (0-10)
  - Relevancia (0-10)
- **Evaluaci√≥n de fuentes**: Precision y recall de documentos recuperados
- **An√°lisis por categor√≠as**: Rendimiento segmentado por temas
- **M√©tricas agregadas**: Accuracy, F1-score, distribuciones de rendimiento

### 3. Suite Completa de Testing (`rag_testing_suite.py`)
- **Pipeline automatizado**: Desde generaci√≥n hasta reporte final
- **Modos de operaci√≥n**: Quick (desarrollo) y Full (evaluaci√≥n completa)
- **Verificaci√≥n de prerequisitos**: Valida configuraci√≥n antes de ejecutar
- **Reportes duales**: JSON detallado y resumen en texto plano
- **Manejo de errores robusto**: Logging completo y recuperaci√≥n de fallos

## üöÄ Uso R√°pido

### Instalaci√≥n y Setup
```bash
# 1. Asegurar que el sistema RAG est√© configurado
python rag_text_processor.py  # Si no se ejecut√≥ antes

# 2. Verificar que existe GOOGLE_API_KEY
export GOOGLE_API_KEY="tu_api_key_aqui"

# 3. Verificar prerequisitos
ls dataset/  # Debe contener archivos .txt
ls greenpeace/greenpeace.csv  # Metadata debe existir
ls chroma_db_rag/  # Base de datos vectorial debe existir
```

### Ejecuci√≥n B√°sica

```bash
# Evaluaci√≥n completa (1000 preguntas)
python rag_testing_suite.py

# Modo r√°pido para desarrollo (50 preguntas)
python rag_testing_suite.py --quick

# N√∫mero espec√≠fico de preguntas
python rag_testing_suite.py --questions 100

# Usar dataset existente (no regenerar)
python rag_testing_suite.py --skip-generation
```

### Ejecuci√≥n por Componentes

```bash
# Solo generar dataset de prueba
python rag_test_generator.py

# Solo ejecutar evaluaci√≥n (requiere test_dataset.json)
python rag_evaluator.py
```

## üìä Outputs y Reportes

### Archivos Generados

1. **`test_dataset.json`**: Dataset de preguntas y respuestas de referencia
2. **`test_dataset_stats.json`**: Estad√≠sticas del dataset generado
3. **`rag_evaluation_report_YYYYMMDD_HHMMSS.json`**: Reporte detallado de evaluaci√≥n
4. **`rag_testing_summary_YYYYMMDD_HHMMSS.txt`**: Resumen ejecutivo en texto plano
5. **Logs**: `rag_testing_suite_YYYYMMDD_HHMMSS.log`

### Estructura del Dataset de Prueba

```json
{
  "id": "qa_0001",
  "question": "¬øCu√°les son las principales consecuencias del cambio clim√°tico seg√∫n el documento?",
  "answer": "Seg√∫n el documento, las principales consecuencias incluyen...",
  "source_paragraph": "El texto original del p√°rrafo...",
  "source_file": "climate-change-impacts.txt",
  "category": "Climate Change",
  "title": "Climate Change Impacts Report",
  "paragraph_length": 456,
  "question_length": 89,
  "answer_length": 234
}
```

### M√©tricas de Evaluaci√≥n

#### M√©tricas Principales (0-10)
- **Similitud Sem√°ntica**: Comparaci√≥n de significado entre respuestas
- **Precisi√≥n Factual**: Correcci√≥n de hechos e informaci√≥n
- **Completitud**: Cobertura de aspectos importantes de la respuesta
- **Relevancia**: Utilidad y pertinencia para la pregunta

#### M√©tricas de Recuperaci√≥n
- **Source Precision**: % de documentos recuperados correctos
- **Source Recall**: % de documentos relevantes recuperados
- **Category Accuracy**: % de coincidencia en categor√≠as

#### M√©tricas de Rendimiento
- **Overall Score**: Puntuaci√≥n ponderada general
- **Score Distribution**: Distribuci√≥n de rendimiento por rangos
- **Error Rate**: % de evaluaciones fallidas
- **Evaluation Time**: Tiempo promedio por pregunta

## üîß Configuraci√≥n y Personalizaci√≥n

### Variables de Configuraci√≥n

En `rag_test_generator.py`:
```python
class RAGTestGenerator:
    def __init__(self, 
                 target_questions: int = 1000,           # N√∫mero de preguntas
                 min_paragraph_length: int = 200,        # Longitud m√≠nima p√°rrafo
                 max_paragraph_length: int = 800):       # Longitud m√°xima p√°rrafo
```

En `rag_evaluator.py`:
```python
class RAGEvaluator:
    def __init__(self, 
                 max_tokens: int = 800,                  # Tokens para LLM juez
                 top_k: int = 5):                        # Chunks recuperados por RAG
```

### Personalizaci√≥n de Prompts

Los prompts para generaci√≥n y evaluaci√≥n est√°n en las clases respectivas y pueden modificarse para:
- Cambiar el estilo de preguntas generadas
- Ajustar criterios de evaluaci√≥n del LLM juez
- Modificar el idioma de evaluaci√≥n
- A√±adir nuevas dimensiones de evaluaci√≥n

### Filtros y Segmentaci√≥n

```python
# Evaluar solo ciertas categor√≠as
evaluator.evaluate_dataset(category_filter="Climate Change")

# Evaluar subset espec√≠fico
evaluator.evaluate_dataset(max_questions=100)
```

## üìà Interpretaci√≥n de Resultados

### Rangos de Puntuaci√≥n

| Rango | Interpretaci√≥n | Acci√≥n Recomendada |
|-------|---------------|-------------------|
| 9-10  | Excelente     | Mantener configuraci√≥n |
| 7-8   | Bueno         | Optimizaciones menores |
| 5-6   | Regular       | Mejoras significativas necesarias |
| 3-4   | Pobre         | Revisi√≥n mayor requerida |
| 0-2   | Muy Pobre     | Reconfiguraci√≥n completa |

### Diagn√≥stico de Problemas

#### Baja Similitud Sem√°ntica
- **Causa**: Respuestas conceptualmente diferentes
- **Soluci√≥n**: Ajustar par√°metros de retrieval, mejorar chunking

#### Baja Precisi√≥n Factual
- **Causa**: Informaci√≥n incorrecta o desactualizada
- **Soluci√≥n**: Validar datos fuente, mejorar prompt del RAG

#### Baja Completitud
- **Causa**: Respuestas muy cortas o incompletas
- **Soluci√≥n**: Aumentar `top_k`, ajustar `max_tokens`

#### Baja Precisi√≥n de Fuentes
- **Causa**: Retrieval recupera documentos irrelevantes
- **Soluci√≥n**: Ajustar similarity threshold, mejorar embeddings

## üîç Casos de Uso

### 1. Desarrollo y Testing
```bash
# Testing r√°pido durante desarrollo
python rag_testing_suite.py --quick

# Testing de cambios espec√≠ficos
python rag_testing_suite.py --questions 25 --skip-generation
```

### 2. Evaluaci√≥n de Producci√≥n
```bash
# Evaluaci√≥n completa antes de deployment
python rag_testing_suite.py --questions 1000

# Evaluaci√≥n continua (usar dataset existente)
python rag_testing_suite.py --skip-generation
```

### 3. An√°lisis Comparativo
```bash
# Evaluar diferentes configuraciones
python rag_testing_suite.py --questions 200  # Config A
# Cambiar par√°metros RAG
python rag_evaluator.py  # Config B con mismo dataset
```

### 4. Debugging Espec√≠fico
```python
# En Python script personalizado
from rag_evaluator import RAGEvaluator

evaluator = RAGEvaluator()
result = evaluator.evaluate_single_question(test_item)
print(f"Detailed scores: {result}")
```

## ‚ö†Ô∏è Limitaciones y Consideraciones

### Limitaciones del Sistema
1. **Dependencia de LLM**: La calidad de evaluaci√≥n depende del LLM juez
2. **Subjetividad**: Algunas m√©tricas pueden tener componente subjetivo
3. **Contexto limitado**: Los p√°rrafos pueden perder contexto al extraerse
4. **Sesgo de generaci√≥n**: Las preguntas reflejan el estilo del LLM generador

### Consideraciones de Costo
- **Modo Quick**: ~$0.50-1.00 USD (50 preguntas)
- **Modo Full**: ~$10.00-20.00 USD (1000 preguntas)
- **Factor principal**: Llamadas a Gemini API para generaci√≥n y evaluaci√≥n

### Recomendaciones de Uso
1. **Usar modo Quick** durante desarrollo activo
2. **Evaluar Full** antes de releases importantes
3. **Validar manualmente** una muestra de resultados
4. **Complementar** con testing humano para casos cr√≠ticos

## üõ†Ô∏è Troubleshooting

### Errores Comunes

#### "ChromaDB not found"
```bash
# Soluci√≥n: Ejecutar procesamiento de texto primero
python rag_text_processor.py
```

#### "GOOGLE_API_KEY not found"
```bash
# Soluci√≥n: Configurar variable de entorno
export GOOGLE_API_KEY="tu_api_key"
# O crear archivo .env con la key
```

#### "No valid paragraphs found"
```bash
# Soluci√≥n: Verificar archivos de dataset
ls dataset/*.txt | wc -l  # Debe ser > 10
```

#### "Error parsing LLM evaluation"
- **Causa**: Respuesta inesperada del LLM juez
- **Soluci√≥n**: Verificar prompts, reintentar con delay

### Performance Issues

#### Evaluaci√≥n muy lenta
```python
# Reducir max_tokens en evaluador
evaluator = RAGEvaluator(max_tokens=400)  # Default: 800

# Usar menos preguntas para testing
python rag_testing_suite.py --questions 50
```

#### Out of memory
```python
# Procesar en lotes m√°s peque√±os
# Modificar batch_size en el c√≥digo si es necesario
```

## üìö Referencias y Recursos Adicionales

- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Google Gemini API](https://ai.google.dev/)
- [RAG Evaluation Best Practices](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/)

---

Este sistema de testing proporciona una evaluaci√≥n robusta y automatizada del sistema RAG, permitiendo iteraci√≥n r√°pida durante el desarrollo y validaci√≥n confiable para producci√≥n.