# Sistema de Testing RAG con LLM Local (Ollama)

## üéØ Resumen

He modificado completamente el sistema de testing del RAG para que use **Ollama** (LLM local) en lugar de Gemini, eliminando as√≠ el costo de API y evitando consumir tu cuota gratuita.

### ‚ú® Beneficios del Cambio

- ‚úÖ **Gratis**: No consume API credits
- ‚úÖ **Privacidad**: Todo local, sin env√≠o de datos
- ‚úÖ **Velocidad**: Una vez configurado, es muy r√°pido
- ‚úÖ **Control**: Modelo corriendo localmente
- ‚ùå **Setup inicial**: Requiere descargar modelo (~2GB)

## üöÄ Instalaci√≥n y Configuraci√≥n

### Opci√≥n 1: Script Autom√°tico (Recomendado)

```bash
# 1. Ejecutar script de instalaci√≥n
./install_ollama.sh

# 2. Probar la configuraci√≥n
conda activate llm
python test_ollama.py

# 3. Ejecutar testing RAG
python rag_testing_suite.py --quick
```

### Opci√≥n 2: Instalaci√≥n Manual

```bash
# 1. Instalar Ollama
# En macOS:
brew install ollama

# En Linux:
curl -fsSL https://ollama.com/install.sh | sh

# 2. Iniciar servidor Ollama
ollama serve &

# 3. Descargar modelo llama3.2
ollama pull llama3.2

# 4. Probar configuraci√≥n
conda activate llm
python test_ollama.py
```

## üìÅ Archivos Modificados

### Nuevos Archivos
- `test_ollama.py` - Test de conectividad con Ollama
- `install_ollama.sh` - Script de instalaci√≥n automatizada
- `RAG_LOCAL_SETUP.md` - Esta documentaci√≥n

### Archivos Modificados
- `rag_test_generator.py` - Usa Ollama en vez de Gemini
- `rag_evaluator.py` - LLM juez con Ollama
- `rag_qa_system.py` - Sistema RAG con Ollama
- `rag_testing_suite.py` - Verificaciones de Ollama
- `requirements.txt` - Dependencias actualizadas

## üîß Uso del Sistema

### 1. Verificar Configuraci√≥n

```bash
conda activate llm
python test_ollama.py
```

**Output esperado:**
```
üöÄ Ollama Test Suite
‚úÖ Ollama server is running
‚úÖ llama3.2 model found
‚úÖ Model test successful
üéâ All tests passed!
```

### 2. Testing R√°pido (Desarrollo)

```bash
python rag_testing_suite.py --quick
```

- Genera 50 preguntas
- Eval√∫a con LLM local
- Tiempo: ~10-15 minutos

### 3. Testing Completo (Producci√≥n)

```bash
python rag_testing_suite.py --questions 1000
```

- Genera 1000 preguntas
- Evaluaci√≥n completa
- Tiempo: ~2-3 horas

### 4. Solo Generaci√≥n de Dataset

```bash
python rag_test_generator.py
```

### 5. Solo Evaluaci√≥n

```bash
python rag_evaluator.py
```

## üéõÔ∏è Configuraci√≥n Avanzada

### Cambiar Modelo Ollama

En los archivos Python, puedes cambiar el modelo:

```python
# De llama3.2 a otro modelo
self.llm = Ollama(
    model="llama3.1",  # o "mistral", "codellama", etc.
    temperature=0.1,
)
```

### Modelos Disponibles

```bash
# Ver modelos instalados
ollama list

# Instalar otros modelos
ollama pull mistral      # M√°s r√°pido, menos preciso
ollama pull llama3.1     # M√°s grande, m√°s preciso
ollama pull codellama    # Especializado en c√≥digo
```

### Ajustar Par√°metros

```python
# En rag_test_generator.py, rag_evaluator.py, etc.
self.llm = Ollama(
    model="llama3.2",
    temperature=0.1,        # 0.0-1.0 (creatividad)
    top_p=0.9,             # Nucleus sampling
    repeat_penalty=1.1,     # Evitar repetici√≥n
)
```

## üìä Comparaci√≥n de Performance

| Aspecto | Gemini (Cloud) | Llama3.2 (Local) |
|---------|----------------|-------------------|
| Costo | $10-20 USD | Gratis |
| Setup | API Key | Instalar Ollama |
| Velocidad | ~2-3 seg/pregunta | ~1-2 seg/pregunta |
| Calidad | Muy alta | Alta |
| Privacidad | Datos enviados | 100% local |
| Disponibilidad | Requiere internet | Offline |

## ‚ö†Ô∏è Troubleshooting

### Error: "Cannot connect to Ollama"

```bash
# Verificar que Ollama est√© corriendo
ps aux | grep ollama

# Si no est√° corriendo:
ollama serve &

# Verificar puerto
curl http://localhost:11434/api/tags
```

### Error: "Model not found"

```bash
# Verificar modelos instalados
ollama list

# Si llama3.2 no aparece:
ollama pull llama3.2
```

### Error: "Out of memory"

```bash
# Usar modelo m√°s peque√±o
ollama pull llama3.2:1b  # Versi√≥n de 1B par√°metros

# O ajustar en Python:
self.llm = Ollama(model="llama3.2:1b")
```

### Performance Lento

```bash
# Verificar recursos del sistema
htop

# Usar modelo m√°s r√°pido
ollama pull tinyllama

# Ajustar par√°metros
self.llm = Ollama(
    model="llama3.2",
    num_predict=100,  # Respuestas m√°s cortas
)
```

## üîÑ Migraci√≥n desde Gemini

Si ten√≠as configurado el sistema anterior:

1. **Backup anterior** (opcional):
   ```bash
   git stash  # Guardar cambios locales
   ```

2. **Usar nueva versi√≥n**:
   ```bash
   git pull   # Actualizar cambios
   ./install_ollama.sh  # Configurar Ollama
   ```

3. **Datasets existentes**:
   Los archivos `test_dataset.json` son compatibles entre versiones

## üìà Resultados Esperados

### M√©tricas T√≠picas con Llama3.2

```
Overall Performance Scores (0-10 scale):
  Semantic Similarity: 7.2 ¬± 1.4
  Factual Accuracy: 7.8 ¬± 1.2  
  Completeness: 6.9 ¬± 1.6
  Relevance: 8.1 ¬± 1.1
  Overall Score: 7.5 ¬± 1.2

Source Matching:
  Source Precision: 72%
  Category Accuracy: 68%
```

### Interpretaci√≥n

- **7.5+ Overall**: Sistema funcionando bien
- **6.0-7.4**: Necesita ajustes menores
- **<6.0**: Requiere mejoras significativas

## üéØ Pr√≥ximos Pasos

Una vez que tengas el sistema funcionando:

1. **Ejecutar test r√°pido** para verificar funcionalidad
2. **Analizar resultados** y ajustar par√°metros si es necesario
3. **Ejecutar evaluaci√≥n completa** para m√©tricas finales
4. **Comparar con baseline** anterior (si lo tienes)

¬øListo para probar? Ejecuta:

```bash
./install_ollama.sh
```

---

*¬øProblemas o preguntas? Revisa el log de Ollama: `tail -f ollama.log`*