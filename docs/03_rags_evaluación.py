# -*- coding: utf-8 -*-
"""03 RAGs - Evaluación.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eVL6xioJOrnOMqdQMMih_BFDy7Tm8yxO

# Evaluación de RAGs

## Entorno
"""

! pip install -qU langchain_huggingface
! pip install -qU langchain-chroma
! pip install -qU langchain[google-genai]

import os

os.environ["GOOGLE_API_KEY"] = ""

"""## *Embeddings* locales"""

from langchain_huggingface import HuggingFaceEmbeddings

# embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

# e = embeddings.embed_query("Hello world")
# print(f"{len(e)=}, {e=}")

# ed = embeddings.embed_documents(["Hello world", "This is an example"])
# print(f"{ed=}")

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

e = embeddings.embed_query("Hello world")
print(f"{len(e)=}, {e=}")

ed = embeddings.embed_documents(["Hello world", "This is an example"])
print(f"{ed=}")

"""## RAG"""

from langchain_core.prompts import PromptTemplate
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chat_models import init_chat_model

class RAG():
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
        self.vector_store = Chroma(collection_name="curso_llm", embedding_function=self.embeddings, persist_directory="./chroma_langchain_db")
        self.llm = init_chat_model("gemini-2.5-flash", model_provider="google_genai")

    def index(self, documents):
        """Cargar los documentos, partirlos en fragmentos, calcular los embeddings e indexarlos."""
        self.documents = documents
        self.vector_store.reset_collection()
        self.vector_store.add_documents(self.documents)

    def retrieve_with_scores(self, query, k=3, filter=None):
        """Encontrar los k documentos más relevantes para una consulta dada. Retornar los documentos y los scores"""
        scores = self.vector_store.similarity_search_with_score(query, k=k, filter=filter)
        return scores

    def retrieve(self, query, k=3, filter=None):
        """Encontrar los k documentos más relevantes para una consulta dada. Retornar los documentos"""
        scores = self.retrieve_with_scores(query, k=k, filter=filter)
        return [s[0] for s in scores] # es s[0] porque scores es una lista de tuplas, con el score en s[1]

    def generate(self, query, relevant_documents):
        """Genera una respuesta para una consulta basándote en los documentos más relevantes."""
        prompt_template = PromptTemplate(
            input_variables=["query", "documents"],
            template = """
            Question: {query}

            Documents:
            {documents}
        """)
        documents = "\n\n".join(doc.page_content for doc in relevant_documents)

        prompt = prompt_template.format(query=query, documents=documents)
        ai_msg = self.llm.invoke([prompt])

        return ai_msg.content

    def answer(self, query):
        """Genera una respuesta para una consulta."""
        relevant_documents = self.retrieve(query)
        answer = self.generate(query, relevant_documents)

        return answer

from langchain_core.documents import Document

documents = [
    Document(page_content="Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity."),
    Document(page_content="Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes."),
    Document(page_content="Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics."),
    Document(page_content="Charles Darwin introduced the theory of evolution by natural selection in his book 'On the Origin of Species'."),
    Document(page_content="Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine."),
    Document(page_content="Galileo Galilei made significant telescopic astronomical observations, including the phases of Venus, the four largest moons of Jupiter, and sunspots, which strongly supported the heliocentric model."),
    Document(page_content="Nikola Tesla was a brilliant inventor best known for his contributions to the design of the modern alternating current (AC) electricity supply system."),
    Document(page_content="Stephen Hawking was a theoretical physicist and cosmologist whose work included black hole research and popularizing science with books like A Brief History of Time."),
    Document(page_content="Rosalind Franklin was a chemist and X-ray crystallographer whose work was crucial to understanding the molecular structures of DNA, RNA, viruses, and coal."),
    Document(page_content="Archimedes was an ancient Greek mathematician, physicist, engineer, inventor, and astronomer who famously discovered the principle of the lever and the buoyancy principle."),
    Document(page_content="Louis Pasteur was a chemist and microbiologist renowned for his discoveries of the principles of vaccination, microbial fermentation, and pasteurization."),
    Document(page_content="Max Planck was a theoretical physicist whose work on quantum theory, for which he won the Nobel Prize in Physics in 1918, laid the foundation for modern physics."),
    Document(page_content="Dmitri Mendeleev is credited with formulating the Periodic Law and creating a published version of the Periodic Table of Elements, which organized the known chemical elements."),
    Document(page_content="Alan Turing was a mathematician, computer scientist, logician, cryptanalyst and philosopher, often considered the father of theoretical computer science and artificial intelligence."),
]

rag = RAG()
rag.index(documents)

query = "Who was the father of artificial intelligence?"
relevant_documents = rag.retrieve(query)
answer = rag.generate(query, relevant_documents)
print(f"{answer=}")

query = "Who is considered the first programmer?"
answer = rag.answer(query)
print(f"{answer=}")

"""## Crear el conjunto de datos de prueba"""

llm = init_chat_model("gemini-2.5-flash-lite", model_provider="google_genai")

"""### Generar preguntas
Generamos preguntas que un usuario podría hacer a partir de un contexto dado
"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class GeneratedQuestion(BaseModel):
    question: str = Field(description="A generated question as a sentence")

initial_question_prompt_template = PromptTemplate(
    input_variables=["context"],
    template="""<instructions>
    Here is some context:
    <context>
    {context}
    </context>
    <role>You are a teacher creating a quiz from a given context.</role>
    <task>
    Your task is to generate 1 question that can be answered using the provided context, following these rules:

    <rules>
    1. The question should make sense to humans even when read without the given context.
    2. The question should be fully answered from the given context.
    3. The question should be framed from a part of context that contains important information. It can also be from tables, code, etc.
    4. The answer to the question should not contain any links.
    5. The question should be of moderate difficulty.
    6. The question must be reasonable and must be understood and responded by humans.
    7. Do not use phrases like 'provided context', etc. in the question.
    8. Avoid framing questions using the word "and" that can be decomposed into more than one question.
    9. The question should not contain more than 10 words, make use of abbreviations wherever possible.
    </rules>

    To generate the question, first identify the most important or relevant part of the context. Then frame a question around that part that satisfies all the rules above.

    Output only the generated question with a "?" at the end, no other text or characters.
    </task>
    </instructions>""")

context = documents[1].page_content

prompt = initial_question_prompt_template.format(context=context)
output = llm.with_structured_output(GeneratedQuestion).invoke([prompt])
question = output.question

print(f"{context=}")
print(f"{question=}")

"""### Generar respuestas
Generamos respuestas a las preguntas generadas en el paso anterior y usando el  mismo contexto
"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class GeneratedAnswer(BaseModel):
    answer: str = Field(description="A generated answer as a sentence")

answer_prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""<instructions>
    <role>You are an experienced QA Engineer for building large language model applications.</role>
    <task>
    It is your task to generate an answer to the following question <question>{question}</question> only based on the <context>{context}</context></task>
    The output should be only the answer generated from the context.

    <rules>
    1. Only use the given context as a source for generating the answer.
    2. Be as precise as possible with answering the question.
    3. Be concise in answering the question and only answer the question at hand rather than adding extra information.
    </rules>

    Only output the generated answer as a sentence. No extra characters.
    </task>
    </instructions>""")

prompt = answer_prompt_template.format(context=context, question=question)

output = llm.with_structured_output(GeneratedAnswer).invoke([prompt])
answer = output.answer

print(f"{context=}")
print(f"{question=}")
print(f"{answer=}")

"""### Extraer los fragmentos relevantes
Para comprobar si una respuesta fue formulada correctamente por el LLM, se obtienen los fragmentos de texto relevantes de los documentos utilizados para responder a las preguntas.
"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class RelevantSources(BaseModel):
    sentences: list[str] = Field(description="Relevant sentences from the given context that can potentially help answer the given question")

source_prompt_template = PromptTemplate(
    input_variables=["full_context", "question"],
    template="""<instructions>
    <role>You are an experienced QA Engineer for building large language model applications.</role>
    <task>
    Your task is to extract the relevant sentences from the given context that can potentially help answer the following question. You are not allowed to make any changes to the sentences from the context.

    Here is the context:
    <context>
    {full_context}
    </context>

    <question>
    {question}
    </question>

    Output only the relevant sentences you found, one sentence per line, without any extra characters or explanations.
    </task>
    </instructions>""")

full_context = "\n\n".join(doc.page_content for doc in documents)
prompt = source_prompt_template.format(full_context=full_context, question=question)
output = llm.with_structured_output(RelevantSources).invoke([prompt])
chunk = output.sentences[0]

print(f"{context=}")
print(f"{question=}")
print(f"{answer=}")
print(f"{chunk=}")

"""### Evolucionar preguntas
Para generar un conjunto de datos de prueba más versátil, evolucionamos las preguntas para ver cómo funciona el RAG frente a preguntas formuladas de manera diferente.
"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class QuestionEvolve(BaseModel):
    question: str = Field(description="Rewritten question with a question mark '?' at the end")

question_evolve_prompt_template = PromptTemplate(
    input_variables=["question"],
    template="""<instructions>
    <role>You are an experienced linguistics expert for building testsets for large language model applications.</role>

    <task>
    It is your task to rewrite the following question in a more indirect and compressed form, following these rules:

    <rules>
    1. Make the question more indirect
    2. Make the question shorter
    3. Use abbreviations if possible
    4. The question should have between 7 and 10 words
    </rules>

    <question>
    {question}
    </question>

    Your output should only be the rewritten question with a question mark "?" at the end. Do not provide any other explanation or text.
    </task>
    </instructions>""")

prompt = question_evolve_prompt_template.format(question=question)
output = llm.with_structured_output(QuestionEvolve).invoke([prompt], temperature=0.5)
evolved_question = output.question

print(f"{context=}")
print(f"{question=}")
print(f"{answer=}")
print(f"{chunk=}")
print(f"{evolved_question=}")

"""### Filtrar preguntas
Revisar y refinar las preguntas, filtrando o modificando las preguntas que no sean relevantes
"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class GroundnessCheck(BaseModel):
    explanation: str = Field(description="Explain your reasoning for the score")
    score: int = Field(descripttion="Your evaluation and reasoning for the rating, from 1 to 5.", gt=0, lt=6)

groundedness_check_prompt_template = PromptTemplate(
    input_variables=["context","question"],
    template="""<instructions>
    <role>You are an experienced linguistics expert for building testsets for large language model applications.</role>

    <task>
    You will be given a context and a question related to that context.

    Your task is to provide an evaluation of how well the given question can be answered using only the information provided in the context.

    <rules>
    Rate this on a scale from 1 to 5, where:
    1 = The question cannot be answered at all based on the given context
    2 = The context provides very little relevant information to answer the question
    3 = The context provides some relevant information to partially answer the question
    4 = The context provides substantial information to answer most aspects of the question
    5 = The context provides all the information needed to fully and unambiguously answer the question
    </rules>

    First, read through the provided context carefully:

    <context>
    {context}
    </context>

    Then read the question:

    <question>
    {question}
    </question>

    Evaluate how well you think the question can be answered using only the context information. Provide your reasoning first in an <evaluation> section, explaining what relevant or missing information from the context led you to your evaluation score in only one sentence.
    </task>
    </instructions>""")

prompt = groundedness_check_prompt_template.format(context=context, question=evolved_question)

output = llm.with_structured_output(GroundnessCheck).invoke([prompt])

explanation = output.explanation
score = output.score

print(f"{score=}")
print(f"{explanation=}")

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class RelevanceCheck(BaseModel):
    explanation: str = Field(description="Explain your reasoning for the score")
    score: int = Field(descripttion="Your evaluation and reasoning for the rating, from 1 to 5.", gt=0, lt=6)

relevance_check_prompt_template = PromptTemplate(
    input_variables=["question"],
    template="""<instructions>
    You will be given a question related to famous scientists. Your task is to evaluate how useful this question would be for a student learning about famous scientists.

    To evaluate the usefulness of the question, consider the following criteria:

    <rules>
    1. Relevance: Is the question directly relevant to your work? Questions that are too broad or unrelated to this domain should receive a lower rating.
    2. Practicality: Does the question address a practical problem or use case that analysts might encounter? Theoretical or overly academic questions may be less useful.
    3. Clarity: Is the question clear and well-defined? Ambiguous or vague questions are less useful.
    4. Depth: Does the question require a substantive answer that demonstrates understanding of financial topics? Surface-level questions may be less useful.
    5. Applicability: Would answering this question provide insights or knowledge that could be applied to real-world company evaluation tasks? Questions with limited applicability should receive a lower rating.
    </rules>

    Here is the question:
    <question>
    {question}
    </question>
    </instructions>""")

prompt = relevance_check_prompt_template.format(question=evolved_question)

output = llm.with_structured_output(RelevanceCheck).invoke([prompt])

explanation = output.explanation
score = output.score

print(f"{score=}")
print(f"{explanation=}")

"""## Funciones de Evaluación"""

sample_queries = [
    "Who introduced the theory of relativity?",
    "Who was the first computer programmer?",
    "What did Isaac Newton contribute to science?",
    "Who won two Nobel Prizes for research on radioactivity?",
    "Who was the father of artificial intelligence?"
]

expected_responses = [
    "Albert Einstein proposed the theory of relativity, which transformed our understanding of time, space, and gravity.",
    "Ada Lovelace is regarded as the first computer programmer for her work on Charles Babbage's early mechanical computer, the Analytical Engine.",
    "Isaac Newton formulated the laws of motion and universal gravitation, laying the foundation for classical mechanics.",
    "Marie Curie was a physicist and chemist who conducted pioneering research on radioactivity and won two Nobel Prizes.",
    "Alan Turing is often considered the father of artificial intelligence'."
]

llm = init_chat_model("gemini-2.5-flash-lite", model_provider="google_genai")

"""### Correctitud"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class CorrectnessGrade(BaseModel):
    ## la explicación se pone antes para forzar al LLM a pensar antes de ver si la respuesta es correcta
    explanation: str = Field(description="Explain your reasoning for the score")
    is_correct: bool = Field(descripttion="True if the answer is correct, False otherwise.")

correctness_instructions = PromptTemplate(
    input_variables=["question", "ground_truth_answer", "answer"],
    template="""<instructions>
<role>You are a teacher grading a quiz.</role>
<task>
You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER.

<rules>
Here is the grade criteria to follow:
1. Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer.
2. Ensure that the student answer does not contain any conflicting statements.
3. It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the ground truth answer.

Here is the correctness criteria:
1. A correctness value of True means that the student's answer meets all of the criteria.
1. A correctness value of False means that the student's answer does not meet all of the criteria.
</rules>

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.

<question>
QUESTION:
{question}
</question>
<ground_truth_answer>
GROUND TRUTH ANSWER: {ground_truth_answer}
</ground_truth_answer>
<answer>
STUDENT ANSWER: {answer}
</answer>
</task>
</instructions>
""")

#question = sample_queries[4]
#ground_truth_answer = expected_responses[4]

question = sample_queries[2]
ground_truth_answer = expected_responses[2]

answer = "Alan Turing is often considered the father of artificial intelligence'."
#answer = "Nikola Tesla is often considered the father of artificial intelligence'."

prompt = correctness_instructions.format(question=question, ground_truth_answer=ground_truth_answer, answer=answer)

output = llm.with_structured_output(CorrectnessGrade).invoke([prompt])
is_correct = output.is_correct
explanation = output.explanation

print(f"{question=}")
print(f"{ground_truth_answer=}")
print(f"{answer=}")
print(f"{is_correct=}")
print(f"{explanation=}")

"""### Relevancia"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class RelevanceGrade(BaseModel):
    explanation: str = Field(description="Explain your reasoning for the score")
    is_relevant: bool = Field(description="True if the answer addresses the question, False otherwise")

relevance_prompt_template = PromptTemplate(
    input_variables=["question", "answer"],
    template="""<instrucions>
<role>You are a teacher grading a quiz. </role>
<task>
You will be given a QUESTION and a STUDENT ANSWER.

<rules>
Here is the grade criteria to follow:
1. Ensure the STUDENT ANSWER is concise and relevant to the QUESTION
2. Ensure the STUDENT ANSWER helps to answer the QUESTION

Here is the relevance criteria:
1. A relevance value of True means that the student's answer meets all of the criteria.
1. A relevance value of False means that the student's answer does not meet all of the criteria.
</rules>
Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.

<question>
QUESTION:
{question}
</question>
<answer>
STUDENT ANSWER: {answer}
</answer>
</task>
</instructions>
</task>
</instrucions>""")

#question = sample_queries[4]
question = sample_queries[2]


answer = "Alan Turing is often considered the father of artificial intelligence'."
#answer = "Nikola Tesla is often considered the father of artificial intelligence'."

prompt = relevance_prompt_template.format(question=question, answer=answer)

output = llm.with_structured_output(RelevanceGrade).invoke([prompt])
is_relevant = output.is_relevant
explanation = output.explanation

print(f"{question=}")
print(f"{answer=}")
print(f"{is_relevant=}")
print(f"{explanation=}")

"""### Fundamentación (*Grounding*)"""

from pydantic import BaseModel, Field
from langchain_core.prompts import PromptTemplate

class GroundedGrade(BaseModel):
    explanation: str = Field(description="Explain your reasoning for the score")
    is_grounded: bool = Field(description="True if the answer is not grounded on the documents, False otherwise")

grounded_prompt_template = PromptTemplate(
    input_variables=["doc_string", "answer"],
    template="""<instructions>
<role>You are a teacher grading a quiz. </role>

<task>
You will be given FACTS and a STUDENT ANSWER.

<rules>
Here is the grade criteria to follow:
1. Ensure the STUDENT ANSWER is grounded in the FACTS.
2. Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.

Ans this is the grounded criteria:
1. A grounded value of True means that the student's answer meets all of the criteria.
2. A grounded value of False means that the student's answer does not meet all of the criteria.
</rules>

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.

<facts>
FACTS:
{doc_string}
</facts>

<answer>
STUDENT ANSWER: {answer}
</answer>
</task>
</instructions>""")

question = sample_queries[4]
#question = sample_queries[2]

relevant_documents = rag.retrieve(question, k=3)
doc_string = "\n\n".join(doc.page_content for doc in relevant_documents)

answer = rag.generate(question, relevant_documents)

prompt = grounded_prompt_template.format(doc_string=doc_string, answer=answer)

output = llm.with_structured_output(GroundedGrade).invoke([prompt])
is_grounded = output.is_grounded
explanation = output.explanation

print(f"{question=}")
print(f"{doc_string=}")
print(f"{answer=}")
print(f"{is_grounded=}")
print(f"{explanation=}")

"""### Relevancia de la recuperación"""

from pydantic import BaseModel, Field

class RetrievalRelevanceGrade(BaseModel):
    explanation: str = Field(description="Explain your reasoning for the score.")
    is_relevant: bool = Field(description="True if the retrieved documents are relevant to the question, False otherwise.")

rounded_prompt_template = PromptTemplate(
    input_variables=["doc_string", "question"],
    template="""<instructions>
<role>You are a teacher grading a quiz.</role>
<task>
Your task is to asses the revelance of a given QUESTION based on the FACTS provided by the student.

<rules>
Here is the grade criteria to follow:
1. You goal is to identify FACTS that are completely unrelated to the QUESTION
2. If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant
3. It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met

And this is the relevance criteria:

1. A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.
2. A relevance value of False means that the FACTS are completely unrelated to the QUESTION.
</rules>

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.

Here are the facts:
<facts>
{doc_string}
</facts>

And this is the question:
<question>{question}</question>
</task>
</instructions>""")

question = sample_queries[4]
#question = sample_queries[2]

relevant_documents = rag.retrieve(question, k=3)

#doc_string = "\n\n".join(doc.page_content for doc in relevant_documents)
doc_string = "\n\n".join(doc.page_content for doc in relevant_documents[-2:])

prompt = rounded_prompt_template.format(doc_string=doc_string, question=question)

output = llm.with_structured_output(RetrievalRelevanceGrade).invoke([prompt])
is_relevant = output.is_relevant
explanation = output.explanation

print(f"{question=}")
print(f"{doc_string=}")
print(f"{is_relevant=}")
print(f"{explanation=}")