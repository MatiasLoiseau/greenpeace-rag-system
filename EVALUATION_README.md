# Sistema de EvaluaciÃ³n del RAG de Greenpeace

Este sistema permite evaluar el rendimiento de tu RAG de manera sistemÃ¡tica y reproducible, generando un conjunto de preguntas de prueba y evaluando mÃºltiples mÃ©tricas de calidad.

## ğŸ“‹ DescripciÃ³n

El sistema consta de dos componentes principales:

1. **`generate_test_questions.py`**: Genera 100 preguntas de evaluaciÃ³n a partir de pÃ¡rrafos aleatorios de los documentos de Greenpeace
2. **`evaluate_rag.py`**: EvalÃºa el RAG usando las preguntas generadas y calcula mÃ©tricas de rendimiento

## ğŸ¯ MÃ©tricas Evaluadas

El sistema evalÃºa 4 mÃ©tricas clave:

1. **Correctitud** (_Correctness_): Â¿La respuesta es factualmente correcta comparada con la respuesta de referencia?
2. **Relevancia** (_Relevance_): Â¿La respuesta es relevante y responde la pregunta?
3. **FundamentaciÃ³n** (_Grounding_): Â¿La respuesta estÃ¡ basada en los documentos recuperados?
4. **Relevancia de RecuperaciÃ³n** (_Retrieval Relevance_): Â¿Los documentos recuperados son relevantes para la pregunta?

## ğŸš€ Uso

### Requisitos Previos

1. **Ollama con Llama 3.1**: AsegÃºrate de tener Ollama corriendo y el modelo instalado:
   ```bash
   # Instalar Ollama si no lo tienes
   # Visita: https://ollama.ai
   
   # Descargar Llama 3.1
   ollama pull llama3.1
   
   # Verificar que estÃ© corriendo
   ollama list
   ```

2. **Entorno Conda** (opcional): Si usas el entorno `llm`:
   ```bash
   conda activate llm
   ```

3. **Dependencias Python**:
   ```bash
   pip install langchain langchain-community langchain-huggingface chromadb ollama pydantic tqdm
   ```

4. **Base de datos ChromaDB**: AsegÃºrate de haber procesado los documentos:
   ```bash
   python process_dataset.py
   ```

### Paso 1: Generar Preguntas de EvaluaciÃ³n

```bash
python generate_test_questions.py
```

Este script:
- Selecciona 100 pÃ¡rrafos aleatorios de los documentos
- Para cada pÃ¡rrafo, genera:
  - Una pregunta original
  - Una respuesta ground truth (respuesta correcta)
  - Fragmentos relevantes del contexto
  - Una pregunta evolucionada (mÃ¡s comprimida e indirecta)
  - MÃ©tricas de calidad (groundedness y relevance scores)
- Guarda todo en `test_questions.json`

**Tiempo estimado**: 30-60 minutos dependiendo de tu hardware y conexiÃ³n con Ollama.

**Nota**: Las preguntas se generan UNA SOLA VEZ. Si el archivo `test_questions.json` ya existe, el script te preguntarÃ¡ si deseas regenerarlas.

### Paso 2: Evaluar el RAG

```bash
# Evaluar con todas las preguntas
python evaluate_rag.py

# Evaluar con un subconjunto (ej: 10 preguntas)
python evaluate_rag.py -n 10

# Cambiar el nÃºmero de documentos recuperados (default: 3)
python evaluate_rag.py -k 5

# Especificar ruta a ChromaDB
python evaluate_rag.py --chroma-path ./mi_chroma_db
```

**Opciones disponibles**:
- `-n, --num-questions`: NÃºmero de preguntas a evaluar (default: todas)
- `-k, --num-docs`: NÃºmero de documentos a recuperar del vector store (default: 3)
- `--chroma-path`: Ruta a la base de datos ChromaDB (default: ./chroma_db)

### Paso 3: Analizar Resultados

Los resultados se guardan en el directorio `evaluation_results/`:

1. **`evaluation_YYYYMMDD_HHMMSS.json`**: Resultados completos en formato JSON
   - MÃ©tricas agregadas
   - Resultados detallados por pregunta
   - Respuestas generadas y documentos recuperados

2. **`summary_YYYYMMDD_HHMMSS.txt`**: Resumen legible en texto plano
   - MÃ©tricas globales
   - Detalles de cada pregunta evaluada
   - Indicadores visuales (âœ…/âŒ) por mÃ©trica

## ğŸ“Š Ejemplo de Salida

```
======================================================================
RESULTADOS
======================================================================

ğŸ“ˆ MÃ©tricas Globales:
   Total de preguntas: 100
   Correctitud: 85.00% (85/100)
   Relevancia: 92.00% (92/100)
   FundamentaciÃ³n: 88.00% (88/100)
   Relevancia de RecuperaciÃ³n: 95.00% (95/100)

ğŸ’¾ Resultados guardados en: evaluation_results/evaluation_20250120_143022.json
ğŸ“„ Resumen guardado en: evaluation_results/summary_20250120_143022.txt
```

## ğŸ”„ Flujo de Trabajo TÃ­pico

### Primera vez:
```bash
# 1. Generar preguntas (una sola vez)
python generate_test_questions.py

# 2. Evaluar el RAG base
python evaluate_rag.py
```

### DespuÃ©s de hacer cambios al RAG:
```bash
# Solo re-evaluar (usa las mismas preguntas)
python evaluate_rag.py
```

### Comparar diferentes configuraciones:
```bash
# Probar con k=3
python evaluate_rag.py -k 3

# Probar con k=5
python evaluate_rag.py -k 5

# Probar con k=10
python evaluate_rag.py -k 10
```

## ğŸ“ Estructura de Archivos

```
greenpeace-rag-system/
â”œâ”€â”€ dataset/                           # Documentos originales
â”œâ”€â”€ chroma_db/                         # Base de datos vectorial
â”œâ”€â”€ generate_test_questions.py         # Script para generar preguntas
â”œâ”€â”€ evaluate_rag.py                    # Script para evaluar el RAG
â”œâ”€â”€ test_questions.json                # Preguntas de evaluaciÃ³n (generado)
â””â”€â”€ evaluation_results/                # Resultados de evaluaciones (generado)
    â”œâ”€â”€ evaluation_YYYYMMDD_HHMMSS.json
    â””â”€â”€ summary_YYYYMMDD_HHMMSS.txt
```

## ğŸ“ MetodologÃ­a

Este sistema de evaluaciÃ³n estÃ¡ basado en las mejores prÃ¡cticas para evaluar sistemas RAG:

1. **GeneraciÃ³n SintÃ©tica de Preguntas**: Usa LLMs para generar preguntas realistas a partir de contextos reales
2. **EvoluciÃ³n de Preguntas**: Crea variaciones mÃ¡s desafiantes de las preguntas originales
3. **Filtrado de Calidad**: Solo incluye preguntas con altos scores de fundamentaciÃ³n y relevancia
4. **EvaluaciÃ³n Multidimensional**: Mide diferentes aspectos del rendimiento del RAG
5. **Reproducibilidad**: Las mismas preguntas se usan en todas las evaluaciones para comparaciones justas

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Cambiar el Modelo LLM

Edita las constantes en los archivos Python:

```python
# En generate_test_questions.py y evaluate_rag.py
MODEL_NAME = "llama3.1"  # Cambia a "llama2", "mistral", etc.
```

### Ajustar ParÃ¡metros de GeneraciÃ³n

En `generate_test_questions.py`:

```python
# Cambiar el nÃºmero de preguntas
test_questions = generate_all_questions(num_questions=200)

# Ajustar temperatura del LLM
llm = Ollama(model=model_name, temperature=0.7)
```

### Ajustar Prompts de EvaluaciÃ³n

Los prompts estÃ¡n definidos como constantes en `evaluate_rag.py`. Puedes modificarlos para ajustar los criterios de evaluaciÃ³n.

## âš ï¸ Troubleshooting

### Error: "Ollama no estÃ¡ corriendo"
```bash
# Iniciar Ollama
ollama serve
```

### Error: "Modelo no encontrado"
```bash
# Descargar el modelo
ollama pull llama3.1
```

### Error: "No se encontrÃ³ ChromaDB"
```bash
# Procesar documentos primero
python process_dataset.py
```

### Las preguntas generadas son de baja calidad
- Ajusta los scores mÃ­nimos en `generate_test_question()`:
  ```python
  if groundedness_score < 4 or relevance_score < 3:  # Ajustar estos valores
  ```

### La evaluaciÃ³n es muy lenta
- Reduce el nÃºmero de preguntas: `python evaluate_rag.py -n 20`
- Usa un modelo mÃ¡s rÃ¡pido (ej: llama2 en lugar de llama3.1)
- Reduce el nÃºmero de documentos recuperados: `python evaluate_rag.py -k 2`

## ğŸ“ˆ Mejores PrÃ¡cticas

1. **Genera las preguntas UNA VEZ**: Usa el mismo conjunto de preguntas para todas tus evaluaciones
2. **Guarda los resultados**: No sobrescribas evaluaciones anteriores, compara mÃºltiples ejecuciones
3. **EvalÃºa incremental**: Empieza con pocas preguntas (-n 10) para probar cambios rÃ¡pidamente
4. **Documenta cambios**: Anota quÃ© cambios hiciste al RAG antes de cada evaluaciÃ³n
5. **Analiza fallos**: Revisa las preguntas donde el RAG fallÃ³ para identificar patrones

## ğŸ“š Referencias

- [LangChain RAG Evaluation](https://python.langchain.com/docs/guides/evaluation/)
- [RAGAS Framework](https://github.com/explodinggradients/ragas)
- Basado en el notebook: `docs/03_rags_evaluaciÃ³n.py`

## ğŸ’¡ PrÃ³ximos Pasos

Posibles mejoras al sistema:

- [ ] Agregar mÃ¡s mÃ©tricas (faithfulness, context recall, etc.)
- [ ] Crear visualizaciones de resultados
- [ ] Implementar comparaciÃ³n automÃ¡tica entre evaluaciones
- [ ] Agregar tests de regresiÃ³n
- [ ] Soportar mÃºltiples LLMs simultÃ¡neamente
- [ ] Crear dashboard interactivo para anÃ¡lisis

---

Â¿Preguntas o problemas? Revisa los logs de ejecuciÃ³n o consulta la documentaciÃ³n de LangChain y Ollama.
